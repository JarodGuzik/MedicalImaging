{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42102bb1-c7fd-43cc-b16f-de7771e5ee8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import directed_hausdorff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d7f04-8eb3-4a57-9284-4e1ab68fd934",
   "metadata": {},
   "source": [
    "# Results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72e7c478-d787-44bc-a614-318366ecfc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes\n",
    "\n",
    "class downNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet1, self).__init__()\n",
    "\n",
    "        # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, 1, padding=1)\n",
    "\n",
    "        # Activation Function\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        # Second 2D convolutional layer, taking in the 64 input layers,\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 1, padding=1)\n",
    "\n",
    "        # Activation Function\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "    \n",
    "class downNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(64, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class downNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet3, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(128, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class downNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(256, 512, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "class upNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1024, 512, 3, 1, padding=1)\n",
    "\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, 1, padding=1)\n",
    "\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "    \n",
    "class upNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class upNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet3, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(256, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class upNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(128, 64, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        # 2 classes, so final output should be 2\n",
    "        self.conv3 = nn.Conv2d(64, 1, 1, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.conv3(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class bottleNeck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bottleNeck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512, 1024, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1024, 1024, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(1024, 512, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "     \n",
    "class pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pool, self).__init__()\n",
    "\n",
    "        self.pooling = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pooling(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "       \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder (Downsampling)\n",
    "        self.down1 = downNet1()\n",
    "        self.pool1 = pool()\n",
    "        \n",
    "        self.down2 = downNet2()\n",
    "        self.pool2 = pool()\n",
    "        \n",
    "        self.down3 = downNet3()\n",
    "        self.pool3 = pool()\n",
    "        \n",
    "        self.down4 = downNet4()\n",
    "        self.pool4 = pool()\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = bottleNeck()\n",
    "        \n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = upNet1()\n",
    "        self.up2 = upNet2()\n",
    "        self.up3 = upNet3()\n",
    "        self.up4 = upNet4()\n",
    "        \n",
    "    def copy_and_crop(self, upsampled, skip):\n",
    "        # Crop the skip connection to match the upsampled dimensions\n",
    "        _, _, H, W = upsampled.size()\n",
    "        skip = skip[:, :, :H, :W]  # Crop the skip connection\n",
    "        return torch.cat((upsampled, skip), dim=1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Downsampling path\n",
    "        \n",
    "        enc1 = self.down1(x)\n",
    "        enc1_pool = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.down2(enc1_pool)\n",
    "        enc2_pool = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.down3(enc2_pool)\n",
    "        enc3_pool = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.down4(enc3_pool)\n",
    "        enc4_pool = self.pool4(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck_out = self.bottleneck(enc4_pool)\n",
    "        \n",
    "        # Upsampling path\n",
    "        copy1 = self.copy_and_crop(bottleneck_out, enc4)\n",
    "        dec1 = self.up1(copy1)\n",
    "         \n",
    "        copy2 = self.copy_and_crop(dec1, enc3)\n",
    "        dec2 = self.up2(copy2)\n",
    "        \n",
    "        copy3 = self.copy_and_crop(dec2, enc2)  # Skip connection\n",
    "        dec3 = self.up3(copy3)\n",
    "        \n",
    "        copy4 = self.copy_and_crop(dec3, enc1)  # Skip connection\n",
    "        dec4 = self.up4(copy4)\n",
    "        \n",
    "        \n",
    "        return dec4\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Transformer\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, C, H, W) -> (H*W, B, C)\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.permute(0, 2, 3, 1).reshape(-1, B, C)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = x.view(H, W, B, C).permute(2, 3, 0, 1)  # (B, C, H, W)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransUNet, self).__init__()\n",
    "\n",
    "        # Encoder (Downsampling)\n",
    "        self.down1 = downNet1()\n",
    "        self.pool1 = pool()\n",
    "        \n",
    "        self.down2 = downNet2()\n",
    "        self.pool2 = pool()\n",
    "        \n",
    "        self.down3 = downNet3()\n",
    "        self.pool3 = pool()\n",
    "        \n",
    "        self.down4 = downNet4()\n",
    "        self.pool4 = pool()\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = bottleNeck()\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer = TransformerEncoder(embed_dim=512, num_heads=8, num_layers=4)\n",
    "\n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = upNet1()\n",
    "        self.up2 = upNet2()\n",
    "        self.up3 = upNet3()\n",
    "        self.up4 = upNet4()\n",
    "\n",
    "    def copy_and_crop(self, upsampled, skip):\n",
    "        # Crop the skip connection to match the upsampled dimensions\n",
    "        _, _, H, W = upsampled.size()\n",
    "        skip = skip[:, :, :H, :W]  # Crop the skip connection\n",
    "        return torch.cat((upsampled, skip), dim=1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Downsampling path\n",
    "        \n",
    "        enc1 = self.down1(x)\n",
    "        enc1_pool = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.down2(enc1_pool)\n",
    "        enc2_pool = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.down3(enc2_pool)\n",
    "        enc3_pool = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.down4(enc3_pool)\n",
    "        enc4_pool = self.pool4(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck_out = self.bottleneck(enc4_pool)\n",
    "\n",
    "        # Apply Transformer\n",
    "        transformer_out = self.transformer(bottleneck_out)\n",
    "\n",
    "        # Upsampling path\n",
    "        copy1 = self.copy_and_crop(transformer_out, enc4)\n",
    "        dec1 = self.up1(copy1)\n",
    "         \n",
    "        copy2 = self.copy_and_crop(dec1, enc3)\n",
    "        dec2 = self.up2(copy2)\n",
    "        \n",
    "        copy3 = self.copy_and_crop(dec2, enc2)  # Skip connection\n",
    "        dec3 = self.up3(copy3)\n",
    "        \n",
    "        copy4 = self.copy_and_crop(dec3, enc1)  # Skip connection\n",
    "        dec4 = self.up4(copy4)\n",
    "        \n",
    "        return dec4\n",
    "\n",
    " ###### Measures #########   \n",
    "    \n",
    "# Dice Coefficient\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    return dice\n",
    "\n",
    "\n",
    "def hausdorff_distance(mask1, mask2):\n",
    "    # Get coordinates of the white pixels (1s)\n",
    "    coords1 = np.argwhere(mask1 == 1)\n",
    "    coords2 = np.argwhere(mask2 == 1)\n",
    "\n",
    "    if coords1.size == 0 or coords2.size == 0:\n",
    "        # If either mask has no white pixels, return infinity\n",
    "        return float('inf')\n",
    "\n",
    "    # Calculate the directed Hausdorff distances\n",
    "    d1 = directed_hausdorff(coords1, coords2)[0]  # Distance from mask1 to mask2\n",
    "    d2 = directed_hausdorff(coords2, coords1)[0]  # Distance from mask2 to mask1\n",
    "\n",
    "    # The Hausdorff distance is the maximum of these two distances\n",
    "    hausdorff_dist = max(d1, d2)\n",
    "\n",
    "    return hausdorff_dist\n",
    "\n",
    "my_nn = TransUNet()\n",
    "# print(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22dc507-7cae-49d9-af98-4578de960b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1073, 0.1061, 0.1025,  ..., 0.1029, 0.1021, 0.0978],\n",
      "         [0.1086, 0.1094, 0.1083,  ..., 0.1020, 0.1032, 0.1019],\n",
      "         [0.1129, 0.1131, 0.1049,  ..., 0.1036, 0.1019, 0.1005],\n",
      "         ...,\n",
      "         [0.1105, 0.1133, 0.1114,  ..., 0.1077, 0.1056, 0.1052],\n",
      "         [0.1111, 0.1085, 0.1132,  ..., 0.1072, 0.1036, 0.1042],\n",
      "         [0.1062, 0.1097, 0.1073,  ..., 0.1056, 0.1045, 0.1035]]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Run a test\n",
    "random_data = torch.rand((1, 1, 224, 224))\n",
    "\n",
    "result = my_nn(random_data)\n",
    "print(result[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef67149-c769-49a2-9ba8-b4b00368270b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classes to load in data\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.image_filenames = os.listdir(images_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
    "        image = cv2.imread(img_path)  # Load image\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "        # Load the corresponding mask\n",
    "        mask_path = os.path.join(self.masks_dir, self.image_filenames[idx])  # Assuming masks have the same filename\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask as grayscale\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Ensure mask is of type long for segmentation\n",
    "        mask = mask.long()  \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, image_dirs, mask_dirs, transform=None):\n",
    "        self.image_dirs = image_dirs\n",
    "        self.mask_dirs = mask_dirs\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "        \n",
    "        # Collect image and mask paths\n",
    "        for img_dir, mask_dir in zip(image_dirs, mask_dirs):\n",
    "            img_files = os.listdir(img_dir)\n",
    "            for img_file in img_files:\n",
    "                img_path = os.path.join(img_dir, img_file)\n",
    "                mask_path = os.path.join(mask_dir, img_file)  # Assume the same naming\n",
    "                if os.path.exists(mask_path):  # Ensure mask exists\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_paths.append(mask_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        # Load images and masks\n",
    "        image = cv2.imread(img_path)  # Load image\n",
    "        \n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask as grayscale\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "            \n",
    "        # Ensure mask is of type long for segmentation\n",
    "        # mask = mask.long()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6e9733-7f5a-4b38-bbd6-c67a0f8fa548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "dataPath = [\n",
    "    'ImagesAndMasks/AllCovid/Covid',\n",
    "    'ImagesAndMasks/AllNormal/Normal',\n",
    "    'ImagesAndMasks/AllOpacity/Lung_Opacity',\n",
    "    'ImagesAndMasks/AllPneumonia/Viral_Pneumonia'   \n",
    "]\n",
    "\n",
    "maskPath = [\n",
    "    'ImagesAndMasks/AllCovid/COVIDmasks',\n",
    "    'ImagesAndMasks/AllNormal/Normalmasks',\n",
    "    'ImagesAndMasks/AllOpacity/Lung_Opacitymasks',\n",
    "    'ImagesAndMasks/AllPneumonia/Viral_Pneumoniamasks'\n",
    "]\n",
    "\n",
    "\n",
    "# Define your transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),               # Convert to PIL Image first\n",
    "    transforms.Resize((224, 224)),         # Resize to 224x224\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),                  # Convert image to tensor\n",
    "])\n",
    "\n",
    "\n",
    "#\n",
    "dataset = MultiDataset(dataPath, maskPath, transform=transform)\n",
    "\n",
    "# Split into train, test, val\n",
    "trainRatio = 0.75\n",
    "valRatio = 0.1\n",
    "testRatio = 1 - trainRatio - valRatio\n",
    "\n",
    "# Calculate the sizes of each split\n",
    "train_size = int(trainRatio * len(dataset))\n",
    "val_size = int(valRatio * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# # Create the DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# print(dataloader.mask)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46e8947f-a842-4239-ad5a-9a0515c79b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1892\n",
      "Validation Loss: 0.0886\n",
      "Epoch [2/10], Loss: 0.0461\n",
      "Validation Loss: 0.0492\n",
      "Epoch [3/10], Loss: 0.0314\n",
      "Validation Loss: 0.0378\n",
      "Epoch [4/10], Loss: 0.0250\n",
      "Validation Loss: 0.0311\n",
      "Epoch [5/10], Loss: 0.0216\n",
      "Validation Loss: 0.0319\n",
      "Epoch [6/10], Loss: 0.0204\n",
      "Validation Loss: 0.0287\n",
      "Epoch [7/10], Loss: 0.0181\n",
      "Validation Loss: 0.0246\n",
      "Epoch [8/10], Loss: 0.0171\n",
      "Validation Loss: 0.0346\n",
      "Epoch [9/10], Loss: 0.0166\n",
      "Validation Loss: 0.0242\n",
      "Epoch [10/10], Loss: 0.0151\n",
      "Validation Loss: 0.0234\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Regular U-Net\n",
    "model = UNet()\n",
    "\n",
    "model.to(device) \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Adjust learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        images = images.float()  # Convert to FloatTensor\n",
    "        masks = masks.float()     # Convert to FloatTensor\n",
    "        # print(f\"Images type: {images.dtype}, shape: {images.shape}\")\n",
    "        # print(f\"Masks type: {masks.dtype}, shape: {masks.shape}\")\n",
    "        # print(images)\n",
    "        \n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # print(f\"Output shape: {outputs.shape}\")\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            images = images.float()  # Convert to FloatTensor\n",
    "            masks = masks.float()\n",
    "            \n",
    "            # print(images.shape)\n",
    "            # print(masks.shape)\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "    \n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d4534c-53f0-4ed3-b3bb-a889291a51d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2262\n",
      "Validation Loss: 0.1390\n",
      "Epoch [2/10], Loss: 0.0869\n",
      "Validation Loss: 0.0835\n",
      "Epoch [3/10], Loss: 0.0676\n",
      "Validation Loss: 0.0950\n",
      "Epoch [4/10], Loss: 0.0562\n",
      "Validation Loss: 0.0505\n",
      "Epoch [5/10], Loss: 0.0473\n",
      "Validation Loss: 0.0457\n",
      "Epoch [6/10], Loss: 0.0420\n",
      "Validation Loss: 0.0440\n",
      "Epoch [7/10], Loss: 0.0381\n",
      "Validation Loss: 0.0453\n",
      "Epoch [8/10], Loss: 0.0353\n",
      "Validation Loss: 0.0363\n",
      "Epoch [9/10], Loss: 0.0323\n",
      "Validation Loss: 0.0314\n",
      "Epoch [10/10], Loss: 0.0304\n",
      "Validation Loss: 0.0369\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# TransU-Net\n",
    "model = TransUNet()\n",
    "\n",
    "model.to(device) \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Adjust learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        images = images.float()  # Convert to FloatTensor\n",
    "        masks = masks.float()     # Convert to FloatTensor\n",
    "        # print(f\"Images type: {images.dtype}, shape: {images.shape}\")\n",
    "        # print(f\"Masks type: {masks.dtype}, shape: {masks.shape}\")\n",
    "        # print(images)\n",
    "        \n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # print(f\"Output shape: {outputs.shape}\")\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            images = images.float()  # Convert to FloatTensor\n",
    "            masks = masks.float()\n",
    "            \n",
    "            # print(images.shape)\n",
    "            # print(masks.shape)\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "    \n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc167293-2c19-4af2-9e37-953308c6ba90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14cdadacf620>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGhCAYAAADbf0s2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6XElEQVR4nO3de1RU9d4/8PfMMDMCwihyGRBE8tajmCgoaiZqSVFYpuWli9rFVXk5mXgq6nTUbvBk2XNOVlZPUT3Z0Vp5q0wDBdGjKALmLV2UKKiMBOoMCAwD8/390Wl+jdxlhr2Heb/W+qzl7L1n7898GXmz9+zZWyGEECAiIpIhpdQNEBERNYchRUREssWQIiIi2WJIERGRbDGkiIhIthhSREQkWwwpIiKSLYYUERHJFkOKiIhkiyFFRESyJWlIvffee4iIiEC3bt0QHR2NPXv2SNkOERHJjGQhtWHDBixZsgQvvvgiCgoKcMsttyAhIQHFxcVStURERDKjkOoCs7GxsRgxYgTef/9927T/+q//wtSpU5GSktLic61WKy5cuAAfHx8oFApnt0pERA4mhEBlZSVCQkKgVDa/v+TRiT3Z1NXVIS8vD88//7zd9Pj4eOzbt6/R8mazGWaz2fb4/PnzGDx4sNP7JCIi5yopKUFoaGiz8yU53FdeXo6GhgYEBQXZTQ8KCoLBYGi0fEpKCnQ6na0YUEREXYOPj0+L8yU9ceLaQ3VCiCYP3yUnJ8NoNNqqpKSks1okIiInau0jG0kO9/n7+0OlUjXaayorK2u0dwUAWq0WWq22s9ojIiKZkGRPSqPRIDo6Gunp6XbT09PTMXbsWClaIiIiGZJkTwoAli5diocffhgxMTEYM2YMPvzwQxQXF+PJJ5+UqiUiIpIZyUJq5syZqKiowMsvv4zS0lJERkZi27ZtCA8Pl6olIiKSGcm+J9URJpMJOp1O6jaIiKiDjEYjfH19m53Pa/cREZFsMaSIiEi2GFJERCRbDCkiIpIthhQREckWQ4qIiGSLIUVERLLFkCIiItliSBERkWwxpIiISLYYUkREJFsMKSIiki2GFBERyRZDioiIZIshRUREssWQIiIi2WJIERGRbDGkiIhIthhSREQkWwwpIiKSLYYUERHJFkOKiIhkiyFFRESyxZAiIiLZYkgREZFsMaSIiEi2GFJERCRbDCkiIpIthhQREckWQ4qIiGTL4SGVkpKCkSNHwsfHB4GBgZg6dSpOnTplt8y8efOgUCjsavTo0Y5uhYiIXJzDQ2r37t1YuHAhcnJykJ6ejvr6esTHx+Pq1at2y91xxx0oLS211bZt2xzdChERuTgPR69w+/btdo/T0tIQGBiIvLw8jB8/3jZdq9VCr9c7evNERNSFOP0zKaPRCADw8/Ozm56VlYXAwEAMHDgQ8+fPR1lZWbPrMJvNMJlMdkVERF2fQgghnLVyIQTuueceXL58GXv27LFN37BhA7p3747w8HAUFRXhpZdeQn19PfLy8qDVahutZ8WKFVi5cqWz2iQiIokYjUb4+vo2v4BwogULFojw8HBRUlLS4nIXLlwQarVafPPNN03Or62tFUaj0VYlJSUCAIvFYrFcvIxGY4v54PDPpP6wePFibN26FdnZ2QgNDW1x2eDgYISHh6OwsLDJ+Vqttsk9LCIi6tocHlJCCCxevBibNm1CVlYWIiIiWn1ORUUFSkpKEBwc7Oh2iIjIhTn8xImFCxfiiy++wJdffgkfHx8YDAYYDAbU1NQAAKqqqrBs2TLs378fZ86cQVZWFqZMmQJ/f3/ce++9jm6HiIhc2fV+3tQcNHPcMS0tTQghRHV1tYiPjxcBAQFCrVaLPn36iLlz54ri4uI2b8NoNEp+HJXFYrFYHa/WPpNy6tl9zmIymaDT6aRug4iIOqi1s/t47T4iIpIthhQREckWQ4qIiGSLIUVERLLFkCIiItliSBERkWwxpIiISLYYUkREJFsMKSIiki2GFBERyRZDioiIZIshRUREssWQIiIi2WJIERGRbDGkiIhIthhSREQkWwwpIiKSLYYUERHJFkOKiIhkiyFFRESy5SF1A0SdTaPRwNfX126a2WxGZWVlk8v7+vpCo9EAAGpqanD16lWn90hEv+OeFLmdiRMnoqCgwK5ef/31Zpd/9913bcv99a9/7cROiYh7UuQ21Go14uPjMWHCBISGhtrNi4qKwowZMxo9R6FQYOjQobblR44cifvvvx+ZmZkoLy/vlL6J3JlCCCGkbqK9TCYTdDqd1G2Qi+nZsycKCgoQHh7eofVYrVZMnDgR2dnZDuqMyH0ZjcZGh9//jHtS1KXddNNNeOGFF6BQKKDRaBAQENDhdSqVSqxYsQJ5eXl44YUXYLFYHNApETWFIUVdkkKhQGBgIIYMGYIZM2ZAoVA4dP0TJ05EcHAw3n//fZSXl8NkMjl0/UT0O544QV2Sl5cXNm/ejPfff9/hAfWHAQMG4MCBA1i4cKFT1k9E3JOiLigmJgYxMTGIiIhw6meXKpUK/v7+8PLycto2iNwdQ4q6nAceeADPPPOM1G0QkQPwcB91GYMHD8aWLVswderUTt3urFmzsHnzZvTv379TtysXQUFBWL9+Pb799lts3boVw4YNk7ol6kK4J0UuT6FQICQkBJGRkUhMTIRS2bl/e/Xv3x/h4eF49dVXO3W7UlCpVAgNDYVKpbJNCw0NxZ133gkfHx8IIfDVV1/Zrt5x5coVXLp0Sap2qSsQDrZ8+XIBwK6CgoJs861Wq1i+fLkIDg4W3bp1E3FxceLYsWPt2obRaGy0DZb7llarFXv27BGVlZWOfju3WV1dnYiJiZF8LJxder1e/PLLL+LKlSu2MplMdmNRWVlpm/fyyy9L3jNL3mU0Glv8v+WUPakhQ4YgIyPD9vjPf3W98cYbWL16NT799FMMHDgQr776KiZPnoxTp07Bx8fHGe1QFxYbG4sxY8agb9++6N69u9TtdEn9+vXDHXfcAeD36xgGBQW1ONZ/njd27NgWz340GAzYuHEjhOtdU4A6i6P/oly+fLkYNmxYk/OsVqvQ6/UiNTXVNq22tlbodDqxdu3aZtdZW1srjEajrUpKSiRPf5b0pVAoxCuvvOLot/B16Wp7UgqFwlYzZsxw2rjt27dPqNVq27akft2szq/W9qSccvC+sLAQISEhiIiIwKxZs3D69GkAQFFREQwGA+Lj423LarVaxMXFYd++fc2uLyUlBTqdzlZhYWHOaJtcyMCBA7F9+3Y89NBDUrfSJSUlJSEjIwMZGRl46aWXnLadwYMHY8eOHcjIyMB3333X4UtWUdfj8MN9sbGx+PzzzzFw4EBcvHgRr776KsaOHYvjx4/DYDAA+P1soD8LCgrC2bNnm11ncnIyli5dantsMpkYVG7Ox8cHEydOhFqtlroVlJeX48KFC6itrZW6lXbx8vJCnz59mjzRZMyYMZg0aZLTe9DpdJg4cSKA32+DctNNNwFAi78PyL04PKQSEhJs/x46dCjGjBmDfv364bPPPsPo0aMBoNEVAIQQLV4VQKvVQqvVOrpVIof45JNPsHLlSpcLqZiYGHz//fdNhpQU/988PT3x1VdfITMzE3fddRc/pyIAnXAKure3N4YOHYrCwkLb91cMBgOCg4Nty5SVlTXauyJqilKpxAMPPIBRo0Z1+qnmzamrq0N1dbXUbbTJ+PHjbX8s9uvXD97e3k67bNT16NatGwYOHIhnn30WQgiYzWZ8+umnMBqNUrdGUnHaJ6L/UVtbK3r37i1WrlxpO3Hiv//7v23zzWZzqydOXIunoLtnKRQK0a1bN3HgwAFnvFWvi8VicZnTrD08POz+77kCo9Eo+vfvL5RKpeTjx3JOtXbihMNDKikpSWRlZYnTp0+LnJwckZiYKHx8fMSZM2eEEEKkpqYKnU4nNm7cKI4ePSpmz54tgoODG33XoiUMKfeshx9+WOTk5Ej6fag/++WXX0RcXJwICwuTfGxaqyFDhog9e/aI8+fPSz1s7VJfXy/y8/PFqlWrJB9DlnOq078nde7cOcyePRvl5eUICAjA6NGjkZOTYztr59lnn0VNTQ0WLFiAy5cvIzY2Fj/++CO/I0Wt6t27N2JjY6Vuw6a6uhoHDhxwic+iunfvjtGjR8PDw7UuMqNSqTB8+HBUVVVhxIgROHv2LCoqKqRuizpTJ/1B5FDck3LPev7556V+69k5cuSI6Natm+Tj0paKjY0VFotF6iG7blarVZjNZjFnzhzJx5Ll2JLkihNEjhQQEIAnnngCEyZMkLoVAIAQAmlpaTh48CDq6+ulbqdZcXFxttPIQ0NDZXOiyfX4487K06dPR3BwMN577z3b9QGpi+ucv4Mci3tS7lVDhgwR1dXVUr/tbBoaGsT48eMlH5fmSqFQCK1WK1asWCH1UDnFb7/9Jvr27Ss8PDwkH2tWx0uSK04QkXQGDBiA7OxsPPHEE1K34hQ9e/bE999/j9dee03qVqgT8HAfyVpkZCSioqJc+lBVZ4qMjER0dDSioqKg0WikbscpVCoVBg8eDIPBgJtvvhmnTp1CeXm51G2Rs3TSHrpD8XCfe5RSqRQ7d+4UDQ0NUr/l7Mj1cJ9KpRJZWVmyGy9nsVqtor6+Xtx///2Sjz3r+ouH+8ilqVQq7kW1gzuNl0KhgEqlwpw5c/DSSy/B29tb6pbICdzj3UwuR61Ww9vb2+5eZERNSUxMxFNPPQV/f394e3vzfdPFMKRIlmbNmoXc3FxER0dL3Qq5gICAAGRkZCAvLw+HDh3CLbfcInVL5CA8cYJkxdPTE6NHj8bo0aMxaNAgqdshF+Hh4YH+/fvbHo8bNw4WiwUHDhyQ9XfZqHUMKZKV3r17Y/PmzfD19ZW6FXJhr7zyCn799VdER0fzCuoujof7SHbkdOsIcl2BgYFYvXo1Zs6cKXUr1AEMKZINb29v+Pr6MqSukxAClZWVvFzQf/j4+ODRRx/Fbbfdhh49erjcxXXpdwwpko01a9Zgy5YtPJX4OlmtVjz66KOYOXMmzGaz1O3IxsyZM5Gfn4+YmBipW6HrwD8tSDaCgoIQGhoqdRstOn36NI4cOSLb20UYDAacP3+et17/Ex8fH3Tv3h233XYbPD09kZ2djYaGBqnbojZiSBG1w9atW/HMM89I3Qa1k0KhwCuvvIITJ05g5MiRqK6ulrolaiMe7iMit9G7d2989NFHPJnChTCkSHLdunVDUFAQtFqt1K24vJ49e6JXr148+aQZOp0ODzzwAMaNG4fAwECeTOECGFIkuXvvvRcFBQW4+eabpW7FpSmVSnzyySf46quvGPitmDdvHg4dOoRhw4ZJ3Qq1gn9GkOS8vLwQHBwsdRsuT6FQwN/fH/7+/lK3Invdu3eHl5cX7rrrLvj7+yM9PR1Wq1XqtqgJDCkicktKpRIrV65Efn4+MjMzUVdXJ3VL1AQe7iPJ9OrVC2lpaViwYIHUrZAbu+GGG7B+/Xrcf//9UrdCTeCeFEnGy8sLU6ZMQa9evaRuhdxYjx49cO+99+Lw4cNSt0JN4J4UERHJFkOKJHHrrbdi1qxZLnMWmslkwueff46cnBypWyEniY6OxiOPPAI/Pz+pW6E/4eE+ksRTTz2F6dOnS91GmxkMBixcuBBVVVVSt0JOcvfdd+P2229HQUEBLl26JHU79B/ckyJqxUsvvYT58+ejpqZG6lbIydRqNd555x28+eabUCr561EOuCdF1Irc3FxkZ2dL3QZ1AqVSiXHjxgHgfc3kgn8qEBGRbHFPijrVwIEDcccdd6B///5St9KqY8eOYefOnTh79qzUrVAn6927NxYvXoysrCyemi4x7klRp4qOjsY//vEPl7hm2v79+7FkyRKcPHlS6laok0VERODtt9/GrbfeKnUrbs/hIdW3b18oFIpGtXDhQgC/X9jx2nmjR492dBtERNQFOPxwX25urt1dL48dO4bJkyfbXXLkjjvuQFpamu2xRqNxdBtERNQFODykAgIC7B6npqaiX79+iIuLs03TarXQ6/VtXqfZbIbZbLY9NplMHW+UiIhkz6mfSdXV1eGLL77Ao48+anc6Z1ZWFgIDAzFw4EDMnz8fZWVlLa4nJSUFOp3OVmFhYc5sm4iIZMKpIbV582ZcuXIF8+bNs01LSEjAunXrsGvXLrz11lvIzc3FpEmT7PaUrpWcnAyj0WirkpISZ7ZNTuLh4QGVSiV1G0RtplKpePdeqQknio+PF4mJiS0uc+HCBaFWq8U333zT5vUajUYBgOVC1bNnT/Htt9+KwsLCjr6tOs2HH34o+bi1p1QqldizZ4/Uw9alnD17VmRmZorw8HDJf75dtYxGY4s/A6f9iXD27FlkZGRg48aNLS4XHByM8PBwFBYWOqsVkgG1Wo3o6GjegZdcSp8+feDn54du3bpJ3YrbctrhvrS0NAQGBuKuu+5qcbmKigqUlJTwlxcRETXilJCyWq1IS0vD3Llz7Y7nVlVVYdmyZdi/fz/OnDmDrKwsTJkyBf7+/rj33nud0QoREbkwpxzuy8jIQHFxMR599FG76SqVCkePHsXnn3+OK1euIDg4GBMnTsSGDRvg4+PjjFaIrovZbIbFYpG6DZIJrVYLtVrN94QEnBJS8fHxEEI0mu7p6YkdO3Y4Y5NEDmMwGHD//ffjzJkzUrdCMuDl5YWvvvoKWVlZeOqpp5r83UbOw3Mria5RV1eHEydO8MZ3BOD323cMGjQIxcXFUrfilniBWSIiki3uSZHTzZ49G3FxcbL+3DE/Px9ffPEFAKCyshLV1dUSd0REAEOKOkFCQgIefvhhqdto0cmTJ/H2229L3QYRXYOH+4iISLYYUkREJFsMKSIiki2GFBGAUaNGYe3atYiKipK6letmtVqxatUqvPrqq6ivr5e6HSKHYEgRAejfvz+eeOIJRERESN3KdRNCYOvWrfj6668ZUg5mMplQVVUldRtuiWf3ERG1oLq6GlOnTsXx48d5tQkJcE+KiKgFVqsVFy5caPUO4uQcDCkiIpItHu4j6iIUCgVefPFFxMbGQqPRSN0OkUMwpIi6CKVSidtvvx3jxo2TupUuo6qqCr/99hsaGhqkbsVt8XAfEVEz3nrrLYwZMwZFRUVSt+K2uCdFRHSN3377DRkZGThw4AAuXrwodTtujSFFRHSNU6dO4eGHH+ZhPhlgSJHTxMTEIDk5GTExMVK34vYKCwvx4osv2n3Jd+jQoVixYgUUCoWEncmLxWLBc889h7y8PFitVqnbITCkyIl69+6NadOmSd1Gm1RXV6O8vBw1NTVSt+IUly5dwqZNm+xCqrS0FI8++ih69eqF7t27S9idfFitVmRmZuLw4cNSt0L/wRMniABs27YNw4cPx86dO6VupdMcOnQII0aMwL/+9S+pWyFqFvekyK3V1NTgm2++QVZWFi5duiR1Ox1itVqxdetWlJWV4Z577oFKpWpx+fr6ely6dAm7du2CRqPBtGnTZH33ZGfLy8vDwYMHUV5eLnUr9GfCBRmNRgGAJfO65557pH6rtKq0tFTo9XrJx8qRddNNN4mamhq715mTkyM8PDyafY6vr6/45ZdfJPopyMPf//53yX927lhGo7HFnwv3pMjt7N+/HytXrgQA1NXVufweFFFXxpAit3Px4kXs2LFD6jZkIyAgAL1794ZarZa6FaJGeOIEkZt79tlnsWfPHoSFhUndClEj3JMiclOhoaGYPn06YmNj3foU9LKyMmzYsAE5OTlSt0JNYEiR21EoFFAqlRBCuPVN7Pr3748333yz1bMAu7pz585h2bJlqKurk7oVagIP95HbGTduHDIzM3HbbbdJ3YqkCgoKMHHiRGzatEnqVoiaxZAih1MqlRg0aBDCw8OlbqVJvXr1wvjx4xEUFCR1K5IyGo3Yu3cvzp8/L3UrRM3i4T5yOF9fX2zZsgU33HCD1K0QkYtr955UdnY2pkyZgpCQECgUCmzevNluvhACK1asQEhICDw9PTFhwgQcP37cbhmz2YzFixfD398f3t7euPvuu3Hu3LkOvRCSF61Wy1OaSdasVis+++wz/O///i+vdi5j7Q6pq1evYtiwYVizZk2T89944w2sXr0aa9asQW5uLvR6PSZPnozKykrbMkuWLMGmTZuwfv167N27F1VVVUhMTOQbhagTKRQKqNVqtz1xwmq1Yu3atXj//ff5u0fOOnIZEQBi06ZNtsdWq1Xo9XqRmppqm1ZbWyt0Op1Yu3atEEKIK1euCLVaLdavX29b5vz580KpVIrt27e3abu8LJK8q0ePHqKoqKgjb61O8dBDD0k+Vs6otl4WKTo6Whw4cEBcvHhRop+AtCwWixg9erTkPy93r9Yui+TQEyeKiopgMBgQHx9vm6bVahEXF4d9+/YB+P0ijhaLxW6ZkJAQREZG2pa5ltlshslksiuSp759+yI6OhparVbqVqgVPj4+iImJQWBgoNStdLrS0lLk5uaiqqpK6laoFQ4NKYPBAACNzpoKCgqyzTMYDNBoNOjZs2ezy1wrJSUFOp3OVvxmvHwlJSVh+/btCA4OlroVomatW7cO48ePb/R5OcmPU05Bv/ZOn0KIVu/+2dIyycnJMBqNtiopKXFYr+RYHh4e8PDgSaMkb1arFfX19W79ZW5X4dCQ0uv1ANBoj6isrMy2d6XX61FXV4fLly83u8y1tFotfH197YqIOsZqtaK6uponDZCsOTSkIiIioNfrkZ6ebptWV1eH3bt3Y+zYsQCA6OhoqNVqu2VKS0tx7Ngx2zJE5Hx5eXkYM2YMNmzYIHUrRM1q93GZqqoq/PLLL7bHRUVFOHz4MPz8/NCnTx8sWbIEr7/+OgYMGIABAwbg9ddfh5eXFx544AEAgE6nw2OPPYakpCT06tULfn5+WLZsGYYOHer2l6khcjQhBAoKCpCfn9/o0NbVq1dx7NgxVFRUSNQdUevaHVKHDh3CxIkTbY+XLl0KAJg7dy4+/fRTPPvss6ipqcGCBQtw+fJlxMbG4scff7S7LfXbb78NDw8PzJgxAzU1Nbj11lvx6aefuu33NYicpb6+Hk888QTy8vL4+Qu5pHaH1IQJE1p8sysUCqxYsQIrVqxodplu3brhnXfewTvvvNPezRNRO4lWrvb+zTff4Ny5c/jrX/8Kf3//TuyMqHW8wCw5hEqlgo+PDzQajdStUDvt3r0ba9euhdFolLoVokZ4rjA5xOjRo5GWlub2VxYnIsdiSJFDeHl5oX///q1+H45cw9GjR20XfQ4ODkZUVJS0DZHbYkgRUSNvv/020tLSAAD3338/vvrqK4k7InfFz6SIqEW5ubl4/PHHkZubK3Ur5IYYUkRuzsvLC35+fs1+BeTMmTP4+OOPcfr06U7ujIghReT2nn/+eezZs4cXbiZZ4mdSRG6uR48eCA0NtZsWExOD3377DTt37kRNTY1EnTleZWUldu7ciaNHj0rdCrURQ4qIGlmwYAFmzpyJqKgo21l+XUFJSQkefPBBVFdXS90KtREP9xG5uc8++wyPPvooysrKpG6FqBGGFJGby8vLwzfffIPKykq76UqlEoGBgejRo4c0jTmBWq2GXq+3u5YoyRtDioia1KNHD+zYsQOrVq2SuhWHueGGG7B//348/fTTUrdCbcSQIiJYLBZs2bIFmZmZtml1dXXIzs7GkSNHAAB79uzBxo0bYTabpWqzw1QqFQIDA7kn5UJ44gQRoaamBklJSUhISLDdiqeqqgpPP/207cSJd999F5s2bcItt9yCgIAAKdslN8KQIrdz4MABpKamIi8vT+pWnOLMmTOYMWMGZs+ejfvuu69dz83Pz8e9994LADCbzSgvL3dGi0RtxpAit1NaWorNmzdL3YbTmEwmfPvtt+jbty9GjhzZrsNzFy9ebHFs6uvrUVJSAqVSiV69ejmgW6KW8TMpoi7qo48+wqhRo/Dzzz87bJ2//fYbJk2ahJdeeslh6yRqCUOK3EZdXR2+/PJL7NixQ+pWOkVtbS0uX76MhoYGh61TCAGj0YiCggJ8+OGHOHv2rMPWTdQUHu4jt1FVVYXk5GQUFxdL3YrLy8nJQU5ODrZs2YI+ffrwPmLkNNyTIrfwz3/+E/fddx+vquBgy5cvx7x581BVVSV1K9RFcU+K3MKJEyfsvgNEjnH48GGUl5fj5MmT6NOnDwIDA6VuiboY7kkRUYecP38eEyZMQGpqqtStUBfEkCKiDhFC4OrVq6itrZW6lTYbOXIklixZguDgYKlboVbwcB8ROYQQAg0NDc3e4VdOJk6ciFtuuQU5OTkoLS2Vuh1qAfekiMghtmzZgkmTJnXZK3mQNLgnRS5PCIHCwkLb4aaQkBD4+/tL3JX7KS0thcFgQF5eHrp3744BAwZAqeTfwdQxfAeRy6urq8ODDz6I2NhYxMbG4ssvv5S6JbclhMCiRYswc+ZMl/qMiuSLe1LUIWq1Go8//jhiY2Ml+0KnEAJ1dXW2X4o//PADTCaT3TI8BNV5LBYLzp8/j9TUVMTFxeHWW2+VuiVyYQwpum4qlQrdu3fHkiVLMHDgQKnbsdm+fTu2b98udRturby8HK+88gqqqqowfvx4eHh48KoUdF14uI+u2+LFi7Fz50706dNH6lZIpr788kuMGzcOR48elboVclHtDqns7GxMmTIFISEhUCgUdpf1t1gseO655zB06FB4e3sjJCQEc+bMwYULF+zWMWHCBCgUCruaNWtWh18MdY7u3btj1KhRGDlyJIYPH45u3bpJ3RLJ1MWLF5GXl4cDBw7Y6o+bKEpNoVAgMjISkZGR3MuTM9FO27ZtEy+++KL45ptvBACxadMm27wrV66I2267TWzYsEGcPHlS7N+/X8TGxoro6Gi7dcTFxYn58+eL0tJSW125cqXNPRiNRgGAJVGNHTtW1NTUiIaGhva+fZyipqZG3HTTTZKPC6v5UqlUwsPDQ3h4eIiXX35Z6reMTX19vThw4IDQaDSSj5G7ltFobPFn1O7PpBISEpCQkNDkPJ1Oh/T0dLtp77zzDkaNGoXi4mK7w0JeXl7Q6/Xt3TzJgEKhgFqt5unF1GZ/vl3Ijz/+CLPZjAULFiAkJETCrn7/XNXDgx/Ny5nTf8sYjUYoFAr06NHDbvq6devg7++PIUOGYNmyZaisrGx2HWazGSaTya6IyDXt3bsXq1atwtmzZ1FTU4OamhrU19dL3RbJlFP/hKitrcXzzz+PBx54AL6+vrbpDz74ICIiIqDX63Hs2DEkJyfjp59+arQX9oeUlBSsXLnSma0SUSeyWCx46KGH4OnpCQBISkrCI488InFXJEdOCymLxYJZs2bBarXivffes5s3f/58278jIyMxYMAAxMTEID8/HyNGjGi0ruTkZCxdutT22GQyISwszFmtUzMUCgWio6MxYsQISb8TVVBQYLc3XVdXx/sZuRghBE6fPm17XF5eLmE3JGdOCSmLxYIZM2agqKgIu3btstuLasqIESOgVqtRWFjYZEhptVpotVpntErtoNFo8OGHHyIqKkqykGpoaMDChQtx8OBBu+lWq1WSfojIuRweUn8EVGFhITIzM9GrV69Wn3P8+HFYLBZeNt8F/PGVASlkZGRg8+bNKCoqYih1MVu2bMHFixexbNkynlBFdtodUlVVVfjll19sj4uKinD48GH4+fkhJCQE9913H/Lz8/Hdd9+hoaEBBoMBAODn5weNRoNff/0V69atw5133gl/f3+cOHECSUlJGD58OG6++WbHvTLqcg4dOoR3331X6jbICf7973+joKAAM2bMgK+vL7y8vDpt2yqVCr6+vqisrITZbO607VIbtfd7BZmZmU2e6z537lxRVFTU7LnwmZmZQgghiouLxfjx44Wfn5/QaDSiX79+4i9/+YuoqKhocw/8npQ0pdVqRUFBQXvfMg6TkpIi+RiwnFcKhUKEhYWJhx56SFit1k57X5nNZvHrr7+KRYsWST4G7lgO/57UhAkTIIRodn5L8wAgLCwMu3fvbu9mSWL9+/dHZGQkdDpdp2+7qqoK//73v3Hq1KlO3zZ1HiEESkpKcP78+U7drkajwQ033AA/P79O3S61Db/FRm3y8MMP4+9//7sk2z5z5gymTZuG6upqSbZPRNLhJQNI9kJDQ/H+++9j+vTpUrdCneDkyZN47LHHsGvXLqlbIRlgSJFkrFYrLl26hIqKClRUVMBisTS5XI8ePTBnzhzExMR0cockhdLSUqSlpSE3NxeXLl3qtDM5vby80KtXL6hUqk7ZHrUNQ4okc/HiRcTFxSEqKgpRUVHYu3ev1C2RjKSmpuLmm2/utKumP/XUUzh48CAGDRrUKdujtuFnUtSiHj16YOLEiRg8eLBD17t//3789NNPOHPmjO1qEc3dbtxkMmHnzp04cuSIQ3sgebty5Qrq6uqwdetWREVFYdy4cU7dnq+vL7RaLTQajVO3Q+3UGad4OhpPQe+8GjFihDCbzQ7/GU6fPr3RtrZt29bkskePHhWenp6SjwVLukpMTHT4e7AptbW1IioqSvLX607V2inoPNxHTVIqlXj55ZeRkpLi0FsZ7Nu3D7NmzcKBAwcazUtNTcWSJUtse1RCCLz88st49tlnUVdX57AeyPXk5+dj1qxZ/PqKG+LhPmqke/fu6NmzJ+68805ER0c7dN1lZWXYs2cPLl261GhednY2iouLsWDBAnTv3h1WqxU//PADcnJyHNoDuZ4LFy5gw4YNiI2NxaBBgxAYGOiU+5kpFAoEBATA39+fF72Vi07Zh3YwHu5zbi1atEiUlZUJi8Xi8J9dbW2tKCsrE4mJiU1uW6VSCX9/fxEQECACAgKEWq2WfDxY8ikfHx9x4403itLSUoe/N/9w6dIl8e233wqVSiX563WHcvgVJ6jr8/LyQkBAgFPWrdVqERAQ0OxV7RsaGvgXLDWrsrIS586dw/r16zF8+HDExcU5fBs9e/ZsdJNWkg4/kyIil1JVVYVnnnmm0X3qqGtiSJFNWFgYvv76a8ybN0/qVohatXfvXtx99938fl0Xx8N9ZOPj44M777zTqbdJqKqqwm+//YarV686bRvkHi5cuIALFy4gPj4e4eHh6N27t8NOpvD09ERERATKysrs7gJNnY97UtSpduzYgeHDh2Pnzp1St0JdxHPPPYf4+HhcvnzZYeuMiorCoUOHMGfOHIetk64P96SoU9TU1OCrr77C7t27YTQapW6HupDq6mpUVla2epug9lCpVNDpdM2e4EOdhyFFnaKyshLJyckoLS2VuhXqgoQQsFqtEEJAoVBI3Q45EA/3EZHLq6iowP33348333xT6lbIwbgnRTZmsxmnTp1CaGjodX9PymAw4MqVK42ml5eXo76+voMdEjXNbDYjOzsbvXr1wsmTJxEaGoru3bt3eL2BgYEYNGgQioqKeGkuqTjta9tOxCtOOKcUCoXw8vISf/vb3677Z/P0008LLy+vJkuhUEj+Gllduzw8PIS3t7f44YcfHPK7xmw2i4sXL4qBAwdK/tq6avGKE9Rmfn5+ePjhhzFmzJh2P/fs2bP4+uuvceDAAd7mnSRTX19vK0fQaDTw8vLi51wSYkiRTVBQEF577bXr+p5UYWEhXnjhBR7SIyKH4okT5BAjR47E7t27cc8990jdChF1IQwpcgidTocxY8ZAr9dL3QoRTp8+jRMnTsBqtXZ4XSqVCoMHD0a/fv0c0Bm1F0OKiLqcpKQkTJ8+HVVVVR1el6enJzZs2IB//vOf/GxKAvxMigAAc+bMQWxsLNRqtdStEHVYfX096urqHHYVCrVazf8bEmFIuTmlUgmNRoNHHnkEEyZMuO71CCFgsVjQ0NDguOaIyO3xcJ+bu++++7B//37ExMR0aD0HDx7EmDFjsGnTJgd1RkTEkHJbWq0Wo0aNQmxsLKKioq772/lWqxUFBQXIyclBQUEB76pLslFbW4uDBw+iqKjIIevr0aMHxo4di6CgIIesj9rIIV/L7mS84kTHq2/fvqKiokI0NDR06GdRWVkpBg8eLJRKpeSvicW6tlQqlVi8eLFDfu9YrVZRX18vnnjiCclfV1cqXnGCmqRUKqFSqTp0k7jvvvsOGRkZMBgMDjnVl8jRGhoaHPbeVCgUUKlUPMOvk7X7N1R2djamTJmCkJAQKBQKbN682W7+vHnzoFAo7Gr06NF2y5jNZixevBj+/v7w9vbG3XffjXPnznXohVDbabVaeHp6tus/m8ViQXV1tV2lp6fjH//4By5duuTEbonInbV7T+rq1asYNmwYHnnkEUyfPr3JZe644w6kpaXZHms0Grv5S5Yswbfffov169ejV69eSEpKQmJiIvLy8qBSqdrbErXT//zP/2Dy5Mnt+hzqgw8+wLvvvms3rayszNGtERHZaXdIJSQkICEhocVltFpts1ceMBqN+Pjjj/F///d/uO222wAAX3zxBcLCwpCRkYHbb7+9vS1RO126dAlnz57F2bNn2/ycgwcP4uTJk07sisg1DBo0COPGjcOhQ4dQW1srdTtdnlM+k8rKykJgYCB69OiBuLg4vPbaawgMDAQA5OXlwWKxID4+3rZ8SEgIIiMjsW/fviZDymw2w2w22x6bTCZntO02/va3v7X7OcKBt+YmcmVPP/005syZgxEjRrTrDz26Pg4/BT0hIQHr1q3Drl278NZbbyE3NxeTJk2yhYzBYIBGo0HPnj3tnhcUFASDwdDkOlNSUqDT6WwVFhbm6LbdihCi3UXkanr16oVVq1ZhxowZDl2vQqGAUqnkCRSdxOF7UjNnzrT9OzIyEjExMQgPD8f333+PadOmNfs8IUSzP/Tk5GQsXbrU9thkMjGoiKhFPj4+mDlz5nXfZZrkwelf5g0ODkZ4eDgKCwsBAHq9HnV1dbh8+bLdcmVlZc1+SU6r1cLX19euiIhacu7cOYwfPx6pqalSt0Id4PTvSVVUVKCkpATBwcEAgOjoaKjVaqSnp9t2w0tLS3Hs2DG88cYbzm6HiNxEfX09zpw5g0OHDuG7776zm9e/f3/ceOON171uDw8PTJw4EUePHsWhQ4c62iq1pL3fuq6srBQFBQWioKBAABCrV68WBQUF4uzZs6KyslIkJSWJffv2iaKiIpGZmSnGjBkjevfuLUwmk20dTz75pAgNDRUZGRkiPz9fTJo0SQwbNkzU19e3qQdecYLFYnWknn322fb+6mvS5s2bJX8trl6tXXGi3SGVmZnZ5Ibmzp0rqqurRXx8vAgICBBqtVr06dNHzJ07VxQXF9uto6amRixatEj4+fkJT09PkZiY2GiZljCkWCxWRyoyMlLMmzdPzJs3T6xYsUJYLJb2/ioUQjCkHFGthZRCCNc7dctkMkGn00ndBhF1ATfddBN++OEH20UHevbs2eaLCmzZsgVTp051Ynddn9FobPE8A167j4jc2s8//4yRI0dCoVBArVbj+++/x+DBg6Vui/6DIUVEbs1iseDChQsAfj8h4rvvvsOxY8fa9FyeNOF8PNxHRESSae1wH296SEREssWQIiIi2WJIERGRbDGkiIhIthhSREQkWwwpIiKSLYYUERHJFkOKiIhkiyFFRESyxZAiIiLZYkgREZFsMaSIiEi2GFJERCRbDCkiIpIthhQREckWQ4qIiGSLIUVERLLFkCIiItliSBERkWwxpIiISLYYUkREJFsMKSIiki2GFBERyRZDioiIZIshRUREssWQIiIi2WJIERGRbDGkiIhIttodUtnZ2ZgyZQpCQkKgUCiwefNmu/kKhaLJWrVqlW2ZCRMmNJo/a9asDr8YIiLqWtodUlevXsWwYcOwZs2aJueXlpba1SeffAKFQoHp06fbLTd//ny75T744IPrewVERNRlebT3CQkJCUhISGh2vl6vt3u8ZcsWTJw4ETfccIPddC8vr0bLNsdsNsNsNtsem0ymdnRMRESuyqmfSV28eBHff/89HnvssUbz1q1bB39/fwwZMgTLli1DZWVls+tJSUmBTqezVVhYmDPbJiIimWj3nlR7fPbZZ/Dx8cG0adPspj/44IOIiIiAXq/HsWPHkJycjJ9++gnp6elNric5ORlLly61PTaZTAwqIiI34NSQ+uSTT/Dggw+iW7dudtPnz59v+3dkZCQGDBiAmJgY5OfnY8SIEY3Wo9VqodVqndkqERHJkNMO9+3ZswenTp3C448/3uqyI0aMgFqtRmFhobPaISIiF+S0kPr4448RHR2NYcOGtbrs8ePHYbFYEBwc7Kx2iIjIBbX7cF9VVRV++eUX2+OioiIcPnwYfn5+6NOnD4DfPzP6+uuv8dZbbzV6/q+//op169bhzjvvhL+/P06cOIGkpCQMHz4cN998cwdeChERdTminTIzMwWARjV37lzbMh988IHw9PQUV65cafT84uJiMX78eOHn5yc0Go3o16+f+Mtf/iIqKira3IPRaGyyBxaLxWK5VhmNxhZ/3yuEEAIuxmQyQafTSd0GERF1kNFohK+vb7Pzee0+IiKSLYYUERHJFkOKiIhkiyFFRESyxZAiIiLZYkgREZFsMaSIiEi2GFJERCRbDCkiIpIthhQREckWQ4qIiGSLIUVERLLFkCIiItliSBERkWwxpIiISLYYUkREJFsMKSIiki2GFBERyRZDioiIZIshRUREssWQIiIi2WJIERGRbDGkiIhIthhSREQkWwwpIiKSLYYUERHJFkOKiIhkiyFFRESyxZAiIiLZYkgREZFstSukUlJSMHLkSPj4+CAwMBBTp07FqVOn7JYRQmDFihUICQmBp6cnJkyYgOPHj9stYzabsXjxYvj7+8Pb2xt33303zp071/FXQ0REXUq7Qmr37t1YuHAhcnJykJ6ejvr6esTHx+Pq1au2Zd544w2sXr0aa9asQW5uLvR6PSZPnozKykrbMkuWLMGmTZuwfv167N27F1VVVUhMTERDQ4PjXhkREbk+0QFlZWUCgNi9e7cQQgir1Sr0er1ITU21LVNbWyt0Op1Yu3atEEKIK1euCLVaLdavX29b5vz580KpVIrt27e3abtGo1EAYLFYLJaLl9FobPH3fYc+kzIajQAAPz8/AEBRUREMBgPi4+Nty2i1WsTFxWHfvn0AgLy8PFgsFrtlQkJCEBkZaVvmWmazGSaTya6IiKjru+6QEkJg6dKlGDduHCIjIwEABoMBABAUFGS3bFBQkG2ewWCARqNBz549m13mWikpKdDpdLYKCwu73raJiMiFXHdILVq0CEeOHMG//vWvRvMUCoXdYyFEo2nXammZ5ORkGI1GW5WUlFxv20RE5EKuK6QWL16MrVu3IjMzE6Ghobbper0eABrtEZWVldn2rvR6Perq6nD58uVml7mWVquFr6+vXRERUdfXrpASQmDRokXYuHEjdu3ahYiICLv5ERER0Ov1SE9Pt02rq6vD7t27MXbsWABAdHQ01Gq13TKlpaU4duyYbRkiIiIAaNfZfU899ZTQ6XQiKytLlJaW2qq6utq2TGpqqtDpdGLjxo3i6NGjYvbs2SI4OFiYTCbbMk8++aQIDQ0VGRkZIj8/X0yaNEkMGzZM1NfX8+w+FovFcqNq7ey+doVUcxtJS0uzLWO1WsXy5cuFXq8XWq1WjB8/Xhw9etRuPTU1NWLRokXCz89PeHp6isTERFFcXNzmPhhSLBaL1TWqtZBS/Cd8XIrJZIJOp5O6DSIi6iCj0djieQa8dh8REckWQ4qIiGSLIUVERLLFkCIiItliSBERkWwxpIiISLYYUkREJFsMKSIiki2GFBERyRZDioiIZIshRUREssWQIiIi2WJIERGRbDGkiIhIthhSREQkWwwpIiKSLYYUERHJFkOKiIhkiyFFRESyxZAiIiLZYkgREZFsMaSIiEi2GFJERCRbDCkiIpIthhQREckWQ4qIiGSLIUVERLLFkCIiItliSBERkWwxpIiISLZcMqSEEFK3QEREDtDa73OXDKnKykqpWyAiIgdo7fe5QrjgbonVasWpU6cwePBglJSUwNfXV+qWXJrJZEJYWBjHsoM4jo7DsXQMOY+jEAKVlZUICQmBUtn8/pJHJ/bkMEqlEr179wYA+Pr6ym7wXRXH0jE4jo7DsXQMuY6jTqdrdRmXPNxHRETugSFFRESy5bIhpdVqsXz5cmi1WqlbcXkcS8fgODoOx9IxusI4uuSJE0RE5B5cdk+KiIi6PoYUERHJFkOKiIhkiyFFRESyxZAiIiLZctmQeu+99xAREYFu3bohOjoae/bskbolWVuxYgUUCoVd6fV623whBFasWIGQkBB4enpiwoQJOH78uIQdy0N2djamTJmCkJAQKBQKbN682W5+W8bNbDZj8eLF8Pf3h7e3N+6++26cO3euE1+FPLQ2lvPmzWv0Hh09erTdMhxLICUlBSNHjoSPjw8CAwMxdepUnDp1ym6ZrvS+dMmQ2rBhA5YsWYIXX3wRBQUFuOWWW5CQkIDi4mKpW5O1IUOGoLS01FZHjx61zXvjjTewevVqrFmzBrm5udDr9Zg8ebLbX8z36tWrGDZsGNasWdPk/LaM25IlS7Bp0yasX78ee/fuRVVVFRITE9HQ0NBZL0MWWhtLALjjjjvs3qPbtm2zm8+xBHbv3o2FCxciJycH6enpqK+vR3x8PK5evWpbpku9L4ULGjVqlHjyySftpt14443i+eefl6gj+Vu+fLkYNmxYk/OsVqvQ6/UiNTXVNq22tlbodDqxdu3aTupQ/gCITZs22R63ZdyuXLki1Gq1WL9+vW2Z8+fPC6VSKbZv395pvcvNtWMphBBz584V99xzT7PP4Vg2raysTAAQu3fvFkJ0vfely+1J1dXVIS8vD/Hx8XbT4+PjsW/fPom6cg2FhYUICQlBREQEZs2ahdOnTwMAioqKYDAY7MZUq9UiLi6OY9qCtoxbXl4eLBaL3TIhISGIjIzk2DYhKysLgYGBGDhwIObPn4+ysjLbPI5l04xGIwDAz88PQNd7X7pcSJWXl6OhoQFBQUF204OCgmAwGCTqSv5iY2Px+eefY8eOHfjoo49gMBgwduxYVFRU2MaNY9o+bRk3g8EAjUaDnj17NrsM/S4hIQHr1q3Drl278NZbbyE3NxeTJk2C2WwGwLFsihACS5cuxbhx4xAZGQmg670vXfJWHQCgUCjsHgshGk2j/y8hIcH276FDh2LMmDHo168fPvvsM9uH0xzT63M948axbWzmzJm2f0dGRiImJgbh4eH4/vvvMW3atGaf585juWjRIhw5cgR79+5tNK+rvC9dbk/K398fKpWqUdqXlZU1+suBmuft7Y2hQ4eisLDQdpYfx7R92jJuer0edXV1uHz5crPLUNOCg4MRHh6OwsJCABzLay1evBhbt25FZmYmQkNDbdO72vvS5UJKo9EgOjoa6enpdtPT09MxduxYibpyPWazGT///DOCg4MREREBvV5vN6Z1dXXYvXs3x7QFbRm36OhoqNVqu2VKS0tx7Ngxjm0rKioqUFJSguDgYAAcyz8IIbBo0SJs3LgRu3btQkREhN38Lve+lOyUjQ5Yv369UKvV4uOPPxYnTpwQS5YsEd7e3uLMmTNStyZbSUlJIisrS5w+fVrk5OSIxMRE4ePjYxuz1NRUodPpxMaNG8XRo0fF7NmzRXBwsDCZTBJ3Lq3KykpRUFAgCgoKBACxevVqUVBQIM6ePSuEaNu4PfnkkyI0NFRkZGSI/Px8MWnSJDFs2DBRX18v1cuSREtjWVlZKZKSksS+fftEUVGRyMzMFGPGjBG9e/fmWF7jqaeeEjqdTmRlZYnS0lJbVVdX25bpSu9LlwwpIYR49913RXh4uNBoNGLEiBG20y+paTNnzhTBwcFCrVaLkJAQMW3aNHH8+HHbfKvVKpYvXy70er3QarVi/Pjx4ujRoxJ2LA+ZmZkCQKOaO3euEKJt41ZTUyMWLVok/Pz8hKenp0hMTBTFxcUSvBpptTSW1dXVIj4+XgQEBAi1Wi369Okj5s6d22icOJaiyTEEINLS0mzLdKX3Je8nRUREsuVyn0kREZH7YEgREZFsMaSIiEi2GFJERCRbDCkiIpIthhQREckWQ4qIiGSLIUVERLLFkCIiItliSBERkWwxpIiISLb+H6O32wq/LGZ1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# turn logit to probability\n",
    "probs = torch.sigmoid(outputs)\n",
    "            \n",
    "# create predictions\n",
    "preds = (probs > 0.5).float()\n",
    "\n",
    "preds_cpu = preds.cpu()\n",
    "\n",
    "# Convert to NumPy array\n",
    "preds_numpy = preds_cpu.numpy()\n",
    "\n",
    "plt.imshow(preds_numpy[1,0,:,:], cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbd93fd7-41b1-4b26-bb52-e73179ea22c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Dice Score is: 0.9696\n",
      "Validation Loss: 0.5597\n"
     ]
    }
   ],
   "source": [
    "# Run test data and apply to measures\n",
    "\n",
    "model.eval()\n",
    "\n",
    "dice = []\n",
    "haus = []\n",
    "\n",
    "with torch.no_grad():\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            images = images.float()  # Convert to FloatTensor\n",
    "            masks = masks.float()\n",
    "            \n",
    "            # print(images.shape)\n",
    "            # print(masks.shape)\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # turn logit to probability\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            \n",
    "            # create predictions\n",
    "            preds = (probs > 0.5).float()\n",
    "            \n",
    "            batch_dice =dice_coefficient(preds, masks)\n",
    "                        \n",
    "            dice.append(batch_dice)\n",
    " \n",
    "# Stack tensors into one\n",
    "dice = torch.stack(dice)  \n",
    "\n",
    "# Calculate average Dice score\n",
    "average_dice = dice.mean().item()  # Get the mean and convert to Python float\n",
    "            \n",
    "print(f'Average Dice Score is: {average_dice:.4f}')\n",
    "print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef1f91-bc43-430a-962b-273ccdf0ab0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf1c09-6f5c-42f3-b788-d720937085da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResUNet\n",
    "\n",
    "class downResNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet1, self).__init__()\n",
    "\n",
    "        # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, 1, padding=1)\n",
    "\n",
    "        # Activation Function\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        # Second 2D convolutional layer, taking in the 64 input layers,\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 1, padding=1)\n",
    "\n",
    "        # Activation Function\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "    \n",
    "class downResNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(64, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class downResNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet3, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(128, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class downResNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(256, 512, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "class upResNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1024, 512, 3, 1, padding=1)\n",
    "\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, 1, padding=1)\n",
    "\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "    \n",
    "class upResNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class upResNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet3, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(256, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class upResNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(128, 64, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        # 2 classes, so final output should be 2\n",
    "        self.conv3 = nn.Conv2d(64, 1, 1, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.conv3(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class ResbottleNeck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bottleNeck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512, 1024, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1024, 1024, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(1024, 512, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "     \n",
    "# class pool(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(pool, self).__init__()\n",
    "\n",
    "#         self.pooling = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.pooling(x)\n",
    "#         output = x\n",
    "#         return output\n",
    "        \n",
    "       \n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder (Downsampling)\n",
    "        self.down1 = downResNet1()\n",
    "        self.pool1 = pool()\n",
    "        \n",
    "        self.down2 = downResNet2()\n",
    "        self.pool2 = pool()\n",
    "        \n",
    "        self.down3 = downResNet3()\n",
    "        self.pool3 = pool()\n",
    "        \n",
    "        self.down4 = downResNet4()\n",
    "        self.pool4 = pool()\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResbottleNeck()\n",
    "        \n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = upResNet1()\n",
    "        self.up2 = upResNet2()\n",
    "        self.up3 = upResNet3()\n",
    "        self.up4 = upResNet4()\n",
    "        \n",
    "    def copy_and_crop(self, upsampled, skip):\n",
    "        # Crop the skip connection to match the upsampled dimensions\n",
    "        _, _, H, W = upsampled.size()\n",
    "        skip = skip[:, :, :H, :W]  # Crop the skip connection\n",
    "        return torch.cat((upsampled, skip), dim=1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Downsampling path\n",
    "        \n",
    "        enc1 = self.down1(x)\n",
    "        enc1_pool = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.down2(enc1_pool)\n",
    "        enc2_pool = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.down3(enc2_pool)\n",
    "        enc3_pool = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.down4(enc3_pool)\n",
    "        enc4_pool = self.pool4(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck_out = self.bottleneck(enc4_pool)\n",
    "        \n",
    "        # Upsampling path\n",
    "        copy1 = self.copy_and_crop(bottleneck_out, enc4)\n",
    "        dec1 = self.up1(copy1)\n",
    "         \n",
    "        copy2 = self.copy_and_crop(dec1, enc3)\n",
    "        dec2 = self.up2(copy2)\n",
    "        \n",
    "        copy3 = self.copy_and_crop(dec2, enc2)  # Skip connection\n",
    "        dec3 = self.up3(copy3)\n",
    "        \n",
    "        copy4 = self.copy_and_crop(dec3, enc1)  # Skip connection\n",
    "        dec4 = self.up4(copy4)\n",
    "        \n",
    "        \n",
    "        return dec4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL6935_Kernel",
   "language": "python",
   "name": "medim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
