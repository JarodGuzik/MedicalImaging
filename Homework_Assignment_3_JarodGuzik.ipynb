{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42102bb1-c7fd-43cc-b16f-de7771e5ee8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdca76-7be4-46db-886c-be153bbff06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72e7c478-d787-44bc-a614-318366ecfc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes\n",
    "\n",
    "class downNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet1, self).__init__()\n",
    "\n",
    "        # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, 1, padding=1)\n",
    "\n",
    "        # Activation Function\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        # Second 2D convolutional layer, taking in the 64 input layers,\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 1, padding=1)\n",
    "\n",
    "        # Activation Function\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "    \n",
    "class downNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(64, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class downNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet3, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(128, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class downNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(256, 512, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "class upNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1024, 512, 3, 1, padding=1)\n",
    "\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, 1, padding=1)\n",
    "\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "    \n",
    "class upNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class upNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet3, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(256, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class upNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(128, 64, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        # 2 classes, so final output should be 2\n",
    "        self.conv3 = nn.Conv2d(64, 1, 1, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.conv3(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class bottleNeck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bottleNeck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512, 1024, 3, 1, padding=1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1024, 1024, 3, 1, padding=1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(1024, 512, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "     \n",
    "class pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pool, self).__init__()\n",
    "\n",
    "        self.pooling = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pooling(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "       \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder (Downsampling)\n",
    "        self.down1 = downNet1()\n",
    "        self.pool1 = pool()\n",
    "        \n",
    "        self.down2 = downNet2()\n",
    "        self.pool2 = pool()\n",
    "        \n",
    "        self.down3 = downNet3()\n",
    "        self.pool3 = pool()\n",
    "        \n",
    "        self.down4 = downNet4()\n",
    "        self.pool4 = pool()\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = bottleNeck()\n",
    "        \n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = upNet1()\n",
    "        self.up2 = upNet2()\n",
    "        self.up3 = upNet3()\n",
    "        self.up4 = upNet4()\n",
    "        \n",
    "    def copy_and_crop(self, upsampled, skip):\n",
    "        # Crop the skip connection to match the upsampled dimensions\n",
    "        _, _, H, W = upsampled.size()\n",
    "        skip = skip[:, :, :H, :W]  # Crop the skip connection\n",
    "        return torch.cat((upsampled, skip), dim=1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Downsampling path\n",
    "        \n",
    "        enc1 = self.down1(x)\n",
    "        enc1_pool = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.down2(enc1_pool)\n",
    "        enc2_pool = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.down3(enc2_pool)\n",
    "        enc3_pool = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.down4(enc3_pool)\n",
    "        enc4_pool = self.pool4(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck_out = self.bottleneck(enc4_pool)\n",
    "        \n",
    "        # Upsampling path\n",
    "        copy1 = self.copy_and_crop(bottleneck_out, enc4)\n",
    "        dec1 = self.up1(copy1)\n",
    "         \n",
    "        copy2 = self.copy_and_crop(dec1, enc3)\n",
    "        dec2 = self.up2(copy2)\n",
    "        \n",
    "        copy3 = self.copy_and_crop(dec2, enc2)  # Skip connection\n",
    "        dec3 = self.up3(copy3)\n",
    "        \n",
    "        copy4 = self.copy_and_crop(dec3, enc1)  # Skip connection\n",
    "        dec4 = self.up4(copy4)\n",
    "        \n",
    "        \n",
    "        return dec4\n",
    "    \n",
    "\n",
    "# Transformer\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, C, H, W) -> (H*W, B, C)\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.permute(0, 2, 3, 1).reshape(-1, B, C)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = x.view(H, W, B, C).permute(2, 3, 0, 1)  # (B, C, H, W)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransUNet, self).__init__()\n",
    "\n",
    "        # Encoder (Downsampling)\n",
    "        self.down1 = downNet1()\n",
    "        self.pool1 = pool()\n",
    "        \n",
    "        self.down2 = downNet2()\n",
    "        self.pool2 = pool()\n",
    "        \n",
    "        self.down3 = downNet3()\n",
    "        self.pool3 = pool()\n",
    "        \n",
    "        self.down4 = downNet4()\n",
    "        self.pool4 = pool()\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = bottleNeck()\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer = TransformerEncoder(embed_dim=512, num_heads=8, num_layers=4)\n",
    "\n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = upNet1()\n",
    "        self.up2 = upNet2()\n",
    "        self.up3 = upNet3()\n",
    "        self.up4 = upNet4()\n",
    "\n",
    "    def copy_and_crop(self, upsampled, skip):\n",
    "        # Crop the skip connection to match the upsampled dimensions\n",
    "        _, _, H, W = upsampled.size()\n",
    "        skip = skip[:, :, :H, :W]  # Crop the skip connection\n",
    "        return torch.cat((upsampled, skip), dim=1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Downsampling path\n",
    "        \n",
    "        enc1 = self.down1(x)\n",
    "        enc1_pool = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.down2(enc1_pool)\n",
    "        enc2_pool = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.down3(enc2_pool)\n",
    "        enc3_pool = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.down4(enc3_pool)\n",
    "        enc4_pool = self.pool4(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck_out = self.bottleneck(enc4_pool)\n",
    "\n",
    "        # Apply Transformer\n",
    "        transformer_out = self.transformer(bottleneck_out)\n",
    "\n",
    "        # Upsampling path\n",
    "        copy1 = self.copy_and_crop(transformer_out, enc4)\n",
    "        dec1 = self.up1(copy1)\n",
    "         \n",
    "        copy2 = self.copy_and_crop(dec1, enc3)\n",
    "        dec2 = self.up2(copy2)\n",
    "        \n",
    "        copy3 = self.copy_and_crop(dec2, enc2)  # Skip connection\n",
    "        dec3 = self.up3(copy3)\n",
    "        \n",
    "        copy4 = self.copy_and_crop(dec3, enc1)  # Skip connection\n",
    "        dec4 = self.up4(copy4)\n",
    "        \n",
    "        return dec4\n",
    "\n",
    "    \n",
    "    \n",
    "# Dice Coefficient\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    return dice\n",
    "\n",
    "my_nn = TransUNet()\n",
    "# print(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e22dc507-7cae-49d9-af98-4578de960b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0704, 0.0713, 0.0661,  ..., 0.0701, 0.0669, 0.0745],\n",
      "         [0.0724, 0.0714, 0.0711,  ..., 0.0694, 0.0722, 0.0796],\n",
      "         [0.0677, 0.0656, 0.0691,  ..., 0.0679, 0.0731, 0.0765],\n",
      "         ...,\n",
      "         [0.0730, 0.0725, 0.0725,  ..., 0.0722, 0.0760, 0.0763],\n",
      "         [0.0709, 0.0717, 0.0727,  ..., 0.0717, 0.0784, 0.0739],\n",
      "         [0.0756, 0.0761, 0.0755,  ..., 0.0781, 0.0788, 0.0750]]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Run a test\n",
    "random_data = torch.rand((1, 1, 224, 224))\n",
    "\n",
    "result = my_nn(random_data)\n",
    "print(result[:,0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ef67149-c769-49a2-9ba8-b4b00368270b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classes to load in data\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.image_filenames = os.listdir(images_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = os.path.join(self.images_dir, self.image_filenames[idx])\n",
    "        image = cv2.imread(img_path)  # Load image\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "        # Load the corresponding mask\n",
    "        mask_path = os.path.join(self.masks_dir, self.image_filenames[idx])  # Assuming masks have the same filename\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask as grayscale\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Ensure mask is of type long for segmentation\n",
    "        mask = mask.long()  \n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class MultiDataset(Dataset):\n",
    "    def __init__(self, image_dirs, mask_dirs, transform=None):\n",
    "        self.image_dirs = image_dirs\n",
    "        self.mask_dirs = mask_dirs\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "        \n",
    "        # Collect image and mask paths\n",
    "        for img_dir, mask_dir in zip(image_dirs, mask_dirs):\n",
    "            img_files = os.listdir(img_dir)\n",
    "            for img_file in img_files:\n",
    "                img_path = os.path.join(img_dir, img_file)\n",
    "                mask_path = os.path.join(mask_dir, img_file)  # Assume the same naming\n",
    "                if os.path.exists(mask_path):  # Ensure mask exists\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.mask_paths.append(mask_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "        \n",
    "        # Load images and masks\n",
    "        image = cv2.imread(img_path)  # Load image\n",
    "        \n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask as grayscale\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "            \n",
    "        # Ensure mask is of type long for segmentation\n",
    "        # mask = mask.long()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f6e9733-7f5a-4b38-bbd6-c67a0f8fa548",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# # Create the DataLoader\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print(dataloader.mask)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "\n",
    "dataPath = [\n",
    "    'ImagesAndMasks/AllCovid/Covid',\n",
    "    'ImagesAndMasks/AllNormal/Normal',\n",
    "    'ImagesAndMasks/AllOpacity/Lung_Opacity',\n",
    "    'ImagesAndMasks/AllPneumonia/Viral_Pneumonia'   \n",
    "]\n",
    "\n",
    "maskPath = [\n",
    "    'ImagesAndMasks/AllCovid/COVIDmasks',\n",
    "    'ImagesAndMasks/AllNormal/Normalmasks',\n",
    "    'ImagesAndMasks/AllOpacity/Lung_Opacitymasks',\n",
    "    'ImagesAndMasks/AllPneumonia/Viral_Pneumoniamasks'\n",
    "]\n",
    "\n",
    "\n",
    "# Define your transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),               # Convert to PIL Image first\n",
    "    transforms.Resize((224, 224)),         # Resize to 224x224\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),                  # Convert image to tensor\n",
    "])\n",
    "\n",
    "\n",
    "#\n",
    "dataset = MultiDataset(dataPath, maskPath, transform=transform)\n",
    "\n",
    "# Split into train, test, val\n",
    "trainRatio = 0.75\n",
    "valRatio = 0.1\n",
    "testRatio = 1 - trainRatio - valRatio\n",
    "\n",
    "# Calculate the sizes of each split\n",
    "train_size = int(trainRatio * len(dataset))\n",
    "val_size = int(valRatio * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# # Create the DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "# print(dataloader.mask)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46e8947f-a842-4239-ad5a-9a0515c79b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1892\n",
      "Validation Loss: 0.0886\n",
      "Epoch [2/10], Loss: 0.0461\n",
      "Validation Loss: 0.0492\n",
      "Epoch [3/10], Loss: 0.0314\n",
      "Validation Loss: 0.0378\n",
      "Epoch [4/10], Loss: 0.0250\n",
      "Validation Loss: 0.0311\n",
      "Epoch [5/10], Loss: 0.0216\n",
      "Validation Loss: 0.0319\n",
      "Epoch [6/10], Loss: 0.0204\n",
      "Validation Loss: 0.0287\n",
      "Epoch [7/10], Loss: 0.0181\n",
      "Validation Loss: 0.0246\n",
      "Epoch [8/10], Loss: 0.0171\n",
      "Validation Loss: 0.0346\n",
      "Epoch [9/10], Loss: 0.0166\n",
      "Validation Loss: 0.0242\n",
      "Epoch [10/10], Loss: 0.0151\n",
      "Validation Loss: 0.0234\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = UNet()\n",
    "\n",
    "model.to(device) \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Adjust learning rate as needed\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Set the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        images = images.float()  # Convert to FloatTensor\n",
    "        masks = masks.float()     # Convert to FloatTensor\n",
    "        # print(f\"Images type: {images.dtype}, shape: {images.shape}\")\n",
    "        # print(f\"Masks type: {masks.dtype}, shape: {masks.shape}\")\n",
    "        # print(images)\n",
    "        \n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # print(f\"Output shape: {outputs.shape}\")\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "    \n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            images = images.float()  # Convert to FloatTensor\n",
    "            masks = masks.float()\n",
    "            \n",
    "            # print(images.shape)\n",
    "            # print(masks.shape)\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "    \n",
    "\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4534c-53f0-4ed3-b3bb-a889291a51d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL6935_Kernel",
   "language": "python",
   "name": "medim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
