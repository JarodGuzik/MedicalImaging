{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42102bb1-c7fd-43cc-b16f-de7771e5ee8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdca76-7be4-46db-886c-be153bbff06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "dataPath = '/blue/eel6935/guzikjar/HW2/LungImages'\n",
    "maskPath = 'LungMasks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e7c478-d787-44bc-a614-318366ecfc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (down1): downNet1(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): ReLU()\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act2): ReLU()\n",
      "  )\n",
      "  (pool1): pool(\n",
      "    (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (down2): downNet2(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): ReLU()\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act2): ReLU()\n",
      "  )\n",
      "  (pool2): pool(\n",
      "    (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (down3): downNet3(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): ReLU()\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act2): ReLU()\n",
      "  )\n",
      "  (pool3): pool(\n",
      "    (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (down4): downNet4(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): ReLU()\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act2): ReLU()\n",
      "  )\n",
      "  (pool4): pool(\n",
      "    (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (bottleneck): bottleNeck(\n",
      "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): ReLU()\n",
      "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act2): ReLU()\n",
      "    (upSamp): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (up1): upNet1(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): ReLU()\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act2): ReLU()\n",
      "    (upSamp): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (up2): upNet2(\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): ReLU()\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act2): ReLU()\n",
      "    (upSamp): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (up3): upNet3(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): ReLU()\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act2): ReLU()\n",
      "    (upSamp): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (up4): upNet4(\n",
      "    (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act1): ReLU()\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (act2): ReLU()\n",
      "    (conv3): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Classes\n",
    "\n",
    "class downNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet1, self).__init__()\n",
    "\n",
    "        # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, 1)\n",
    "\n",
    "        # Activation Function\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        # Second 2D convolutional layer, taking in the 64 input layers,\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 1)\n",
    "\n",
    "        # Activation Function\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "    \n",
    "class downNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(64, 128, 3, 1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, 1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class downNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet3, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(128, 256, 3, 1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class downNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(downNet4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(256, 512, 3, 1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, 1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "class upNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1024, 512, 3, 1)\n",
    "\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(512, 512, 3, 1)\n",
    "\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(512, 256, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "\n",
    "    \n",
    "class upNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512, 256, 3, 1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, 3, 1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(256, 128, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class upNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet3, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(256, 128, 3, 1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 128, 3, 1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(128, 64, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class upNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upNet4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(128, 64, 3, 1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        # 2 classes, so final output should be 2\n",
    "        self.conv3 = nn.Conv2d(64, 2, 1, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.conv3(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "        \n",
    "class bottleNeck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bottleNeck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512, 1024, 3, 1)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1024, 1024, 3, 1)\n",
    "        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.upSamp = nn.ConvTranspose2d(1024, 512, 2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.upSamp(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "     \n",
    "class pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pool, self).__init__()\n",
    "\n",
    "        self.pooling = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pooling(x)\n",
    "        output = x\n",
    "        return output\n",
    "        \n",
    "       \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder (Downsampling)\n",
    "        self.down1 = downNet1()\n",
    "        self.pool1 = pool()\n",
    "        \n",
    "        self.down2 = downNet2()\n",
    "        self.pool2 = pool()\n",
    "        \n",
    "        self.down3 = downNet3()\n",
    "        self.pool3 = pool()\n",
    "        \n",
    "        self.down4 = downNet4()\n",
    "        self.pool4 = pool()\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = bottleNeck()\n",
    "        \n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = upNet1()\n",
    "        self.up2 = upNet2()\n",
    "        self.up3 = upNet3()\n",
    "        self.up4 = upNet4()\n",
    "        \n",
    "    def copy_and_crop(self, upsampled, skip):\n",
    "        # Crop the skip connection to match the upsampled dimensions\n",
    "        _, _, H, W = upsampled.size()\n",
    "        skip = skip[:, :, :H, :W]  # Crop the skip connection\n",
    "        return torch.cat((upsampled, skip), dim=1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Downsampling path\n",
    "        enc1 = self.down1(x)\n",
    "        enc1_pool = self.pool1(enc1)\n",
    "        \n",
    "        enc2 = self.down2(enc1_pool)\n",
    "        enc2_pool = self.pool2(enc2)\n",
    "        \n",
    "        enc3 = self.down3(enc2_pool)\n",
    "        enc3_pool = self.pool3(enc3)\n",
    "        \n",
    "        enc4 = self.down4(enc3_pool)\n",
    "        enc4_pool = self.pool4(enc4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck_out = self.bottleneck(enc4_pool)\n",
    "        \n",
    "        # Upsampling path\n",
    "        dec1 = self.up1(bottleneck_out)\n",
    "        dec1 = self.copy_and_crop(dec1, enc4)  # Skip connection\n",
    "        \n",
    "        dec2 = self.up2(dec1)\n",
    "        dec2 = self.copy_and_crop(dec2, enc3)  # Skip connection\n",
    "        \n",
    "        dec3 = self.up3(dec2)\n",
    "        dec3 = self.copy_and_crop(dec3, enc2)  # Skip connection\n",
    "        \n",
    "        dec4 = self.up4(dec3)\n",
    "        dec4 = self.copy_and_crop(dec4, enc1)  # Skip connection\n",
    "        \n",
    "        return dec4\n",
    "    \n",
    "           \n",
    "\n",
    "my_nn = UNet()\n",
    "print(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22dc507-7cae-49d9-af98-4578de960b4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [512, 1024, 3, 3], expected input[1, 512, 56, 56] to have 1024 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run a test\u001b[39;00m\n\u001b[1;32m      2\u001b[0m random_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m572\u001b[39m, \u001b[38;5;241m572\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmy_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m (result)\n",
      "File \u001b[0;32m/blue/eel6935/guzikjar/conda/envs/MedIm/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/eel6935/guzikjar/conda/envs/MedIm/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 275\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m bottleneck_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbottleneck(enc4_pool)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Upsampling path\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m dec1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbottleneck_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m dec1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_and_crop(dec1, enc4)  \u001b[38;5;66;03m# Skip connection\u001b[39;00m\n\u001b[1;32m    278\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(dec1)\n",
      "File \u001b[0;32m/blue/eel6935/guzikjar/conda/envs/MedIm/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/eel6935/guzikjar/conda/envs/MedIm/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 107\u001b[0m, in \u001b[0;36mupNet1.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact1(x)\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n",
      "File \u001b[0;32m/blue/eel6935/guzikjar/conda/envs/MedIm/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/eel6935/guzikjar/conda/envs/MedIm/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/blue/eel6935/guzikjar/conda/envs/MedIm/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/blue/eel6935/guzikjar/conda/envs/MedIm/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [512, 1024, 3, 3], expected input[1, 512, 56, 56] to have 1024 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "# Run a test\n",
    "random_data = torch.rand((1, 1, 572, 572))\n",
    "\n",
    "result = my_nn(random_data)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e9733-7f5a-4b38-bbd6-c67a0f8fa548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL6935_Kernel",
   "language": "python",
   "name": "medim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
